{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4909936",
   "metadata": {},
   "source": [
    "# Homework 4: Vector Semantics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d12a41",
   "metadata": {},
   "source": [
    "Have you ever fantasized about writing a program to take quizzes or tests for you?\n",
    "\n",
    "In this assignment, you’ll leverage dense word embeddings to write a program that can answer various multiple choice and true/false quiz questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b3f6e6",
   "metadata": {},
   "source": [
    "## Organization and Instructions\n",
    "Execute the code cells in Part 1 to understand the background for this assignment. You will not need to modify or add anything to Part 1. Part 2 is where your solution begins.\n",
    "\n",
    "**Part 1: Background.** \n",
    "- 1A. Environment set-up \n",
    "- 1B. Data exploration \n",
    "\n",
    "**Part 2: Your implementation.** \n",
    "- 2A. Similarity/distance metrics \n",
    "- 2B. Synonym Questions\n",
    "- 2C. Analogies \n",
    "- 2D. Sentence Similarity\n",
    "- 2E. Gendered Occupations\n",
    "\n",
    "**(Optional) Part 3: Extra Credit.** \n",
    "Extra credit can only help you and will not hurt you. At the end of the semester, if you have a borderline grade, your grade could be increased given your efforts on extra credit. This section is intended to be open-ended and challenge you. We suggest you only attempt this section after you have completed all other parts and are satisifed with your submission.\n",
    "\n",
    "**Addtional instructions.** \n",
    "- Your submitted solution and code must be yours alone. Copying and pasting a solution from the internet or another source is considered a violation of the honor code. \n",
    "- However, you can talk to classmates about *high-level* approaches. In the **Process Reporting** section, record the names of any classmates you spoke with about this assignment. \n",
    "\n",
    "**Evaluation.** Your solution will be evaluated on a mix of: \n",
    "- Unit tests \n",
    "- Accuracy on the dev set which we have provided to you \n",
    "- Accuracy on the held-out test set (scores seen on Gradescope) \n",
    "- Manually-graded free response questions. \n",
    "\n",
    "**Grading.**\n",
    "For accuracy scores, your grade will be a proportion of target accuracy scores as they have been on previous assignments. \n",
    "\n",
    "- Part 2A. Similarity/distance metrics  \n",
    "    - **5 points (autograded).** `euclidean_distance()` unit tests \n",
    "    - **5 points (autograded).** `cosine_similarity()` unit tests \n",
    "- Part 2B. Synonym Questions \n",
    "    - **5 points (autograded).** Dev Acc=`0.667` using euclidean distance \n",
    "    - **5 points (autograded).** Dev Acc=`0.833` using cosine similarity \n",
    "    - **5 points (autograded).** Test Acc=`0.720` using euclidean distance \n",
    "    - **5 points (autograded).** Test Acc=`0.840` using cosine similarity \n",
    "    - **5 points (manually graded).** TAs and the instructor will manually grade your error analysis response. \n",
    "- Part 2C. Analogies \n",
    "    - **5 points (autograded).** Dev Acc=`0.640`\n",
    "    - **5 points (autograded).** Test Acc=`0.767`\n",
    "- Part 2D. Sentence Similarity \n",
    "    - **5 points (autograded).** Dev Acc=`0.780`\n",
    "    - **5 points (autograded).** Test Acc=`0.816`\n",
    "- Part 2E. Gendered Occupations \n",
    "    - **10 points (autograded).** Checks correct five most similar occupations for both \"man\" and \"woman\" \n",
    "    - **5 points (manually graded).** TAs and the instructor will manually grade your response to the free-response question. \n",
    "- Style. \n",
    "    - **5 points (manually graded).** TAs and the instructor will evaluate your submission on style. Are you using the best practices you have learned in previous classes and we discussed during lecture? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2d23b9",
   "metadata": {},
   "source": [
    "### 1A. Environment Set-up "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d90943",
   "metadata": {},
   "source": [
    "If you set-up your conda environment correctly in HW0, you should see `Python [conda env:cs375]` as the kernel in the upper right-hand corner of the Jupyter webpage you are currently on. Run the cell below to make sure your environment is correctly installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e09ae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment check \n",
    "# Return to HW0 if you run into errors in this cell \n",
    "# Do not modify this cell \n",
    "import os\n",
    "assert os.environ['CONDA_DEFAULT_ENV'] == \"cs375\"\n",
    "\n",
    "import sys\n",
    "assert sys.version_info.major == 3 and sys.version_info.minor == 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e5b7b0",
   "metadata": {},
   "source": [
    "If there are any errors after running the cell above, return to the instructions from `HW0`. If you are still having difficulty, reach out to the instructor or TAs via Piazza. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2065bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules for this assignment \n",
    "# Do not modify this cell \n",
    "import numpy as np\n",
    "from typing import List, Dict, Union, Tuple\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import re\n",
    "import nltk\n",
    "import quizlet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a9cc01",
   "metadata": {},
   "source": [
    "### 1B. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92cfdf2",
   "metadata": {},
   "source": [
    "**Embeddings.** Here, we provide you with dense embeddings for 4196 words. These are a subset of [GloVe embeddings](https://nlp.stanford.edu/projects/glove/), which are trained on many hundreds of thousands of Wikipedia articles in a very similar way to `word2vec` embeddings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343a2291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the gensim package to load the embeddings \n",
    "embeddings = KeyedVectors.load_word2vec_format(\"data/embeddings/glove50_4k.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245cee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We gave you a subset of ~4k words \n",
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445ecef5",
   "metadata": {},
   "source": [
    "Although not quite a dictionary you can think of this embedding variable as being like a dictionary in that you obtain an embedding by calling it with the word type of interest (as a string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9d3f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "king_embedding = embeddings['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea92e5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Embedding type=\", type(king_embedding))\n",
    "print()\n",
    "print(\"Embeding =\", king_embedding)\n",
    "print()\n",
    "print(\"Shape of embedding=\", king_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57253bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like dictionaries, you can check if a word is in the embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c272eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"trick\" in embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a199eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"tricks\" in embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c29d31",
   "metadata": {},
   "source": [
    "### 2A. Similarity/distance metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8638f12",
   "metadata": {},
   "source": [
    "First, you’ll first implement two similarity/distance metrics -- `cosine_similarity()` and `euclidean_distance()`. You will leverate these functions to answer the multiple choice questions in the parts below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572858cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1: np.ndarray, v2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculates and returns the cosine similarity between word embeddings v1 and v2\n",
    "    \n",
    "    Arguments:\n",
    "        - v1 (np.array), v2 (np.array): word embeddings \n",
    "    Returns:\n",
    "        - float: the cosine similarity between v1, v2\n",
    "        \n",
    "    Hints: \n",
    "        - You should be using numpy functions here. Look through the numpy \n",
    "        documentation to find with functions might be helpful here.  \n",
    "    \"\"\"\n",
    "    # TODO: implement your solution here \n",
    "    # CODE START\n",
    "    raise NotImplementedError(\"Solution not yet implemented!\") #delete this line and add your solution\n",
    "    # CODE END  \n",
    "\n",
    "def euclidean_distance(v1: np.ndarray, v2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculates and returns the euclidean distance between v1 and v2\n",
    "\n",
    "    Arguments:\n",
    "        - v1 (np.array), v2 (np.array): v\n",
    "\n",
    "    Returns:\n",
    "        - float: the euclidean distance between v1, v2\n",
    "        \n",
    "    Hint: \n",
    "        - You should be using numpy functions here. Look through the numpy \n",
    "        documentation to find with functions might be helpful here.  \n",
    "    \"\"\"\n",
    "    # TODO: implement your solution here \n",
    "    # CODE START\n",
    "    raise NotImplementedError(\"Solution not yet implemented!\") #delete this line and add your solution\n",
    "    # CODE END "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffda69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does your code work for the toy example below? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec6b359",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_embeddings ={\n",
    "    \"cherry\": np.array([442, 8, 2]), \n",
    "    \"digital\": np.array([5, 1683, 1670]), \n",
    "    \"information\": np.array([8, 3982, 3325])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7373951",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(toy_embeddings[\"cherry\"], toy_embeddings[\"digital\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1801fa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_distance(toy_embeddings[\"cherry\"], toy_embeddings[\"digital\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47e2dfd",
   "metadata": {},
   "source": [
    "### 2B. Synonym Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e1bbd5",
   "metadata": {},
   "source": [
    "For this section, the input is a word and a list of candidate choices. Your goal is to return the choice that is the synonym. Your implementation will answer questions similar to the following example: \n",
    "```\n",
    "  What is a synonym for warrior?  \n",
    "  a) soldier  \n",
    "  b) sailor  \n",
    "  c) pirate  \n",
    "  d) spy  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea34c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_synonym(word: str, \n",
    "                 choices: List[str], \n",
    "                 embeddings: Dict[str, np.array], \n",
    "                 comparison_metric:str) -> str:\n",
    "    \"\"\"\n",
    "    Answer a multiple choice synonym question. Namely, given a target word \n",
    "    and list of choices (candidate answers), find the word that is most \n",
    "    similar the target word. Similarity will be determined by either euclidean distance \n",
    "    or cosine similarity, depending on what is passed in as the comparison_metric.\n",
    "\n",
    "    Arguments:\n",
    "        - word (str): the word that is your target to find a synonym for \n",
    "        - choices (List[str]): list of candidates for synonyms\n",
    "        - embeddings (Dict[str, np.array]): map of words to their embeddings\n",
    "        - comparison_metric (str): either 'euc_dist' or 'cosine_sim'. \n",
    "            This indicates which metric to use - either euclidean distance or cosine similarity.\n",
    "            With euclidean distance, we want the word with the lowest euclidean distance.\n",
    "            With cosine similarity, we want the word with the highest cosine similarity.\n",
    "\n",
    "    Returns:\n",
    "        - str: the item in choices that is most similar to the target word\n",
    "        \n",
    "    Hints: \n",
    "        - You should be calling cosine_similarity() and euclidean_distance(), the functions\n",
    "        you previously implemented, during this function \n",
    "    \"\"\"\n",
    "    assert comparison_metric in ['euc_dist', 'cosine_sim']\n",
    "    \n",
    "    # TODO: implement your solution here \n",
    "    # CODE START\n",
    "    raise NotImplementedError(\"Solution not yet implemented!\") #delete this line and add your solution\n",
    "    # CODE END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8be875",
   "metadata": {},
   "source": [
    "#### Toy example for code development "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5403550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does your code work for this toy example? \n",
    "toy_embeddings ={\n",
    "    \"cherry\": np.array([442, 8, 2]), \n",
    "    \"digital\": np.array([5, 1683, 1670]), \n",
    "    \"information\": np.array([8, 3982, 3325])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89bb6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_synonym(\"digital\", [\"cherry\", \"information\"], toy_embeddings, \"euc_dist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26612561",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_synonym(\"digital\", [\"cherry\", \"information\"], toy_embeddings, \"cosine_sim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddd9bd5",
   "metadata": {},
   "source": [
    "#### Evaluating on 30 multiple choice questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25646f0b",
   "metadata": {},
   "source": [
    "The code cell below runs your implementation on 30 multiple choice questions about **synonyms**. \n",
    "\n",
    "Our reference implementation achieved accuracies of: \n",
    "- `0.667` using euclidean distance \n",
    "- `0.833` using cosine similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b018db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell \n",
    "part2B = quizlet.Part2B_Runner(find_synonym)\n",
    "_ = part2B.evaluate(True)  # To only print the scores, pass in False as an argument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a035c6b",
   "metadata": {},
   "source": [
    "#### (5 points) Error analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c99163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell \n",
    "find_synonym(\"sanguine\", [\"pessimistic\", \"unsure\", \"sad\", \"positive\"], \n",
    "             embeddings, 'cosine_sim')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69d6917",
   "metadata": {},
   "source": [
    "Finding synonyms using cosine similarity on word embeddings does fairly well. However, it's not perfect. In particular, you should see that it predicts an incorrect answer for the question in the cell above (Question 30 in your multiple choice test). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc71f02",
   "metadata": {},
   "source": [
    "In *at least two sentences*, explain why you think your implementation answered incorrectly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb00400",
   "metadata": {},
   "source": [
    "*DELETE AND PUT YOUR ANSWER HERE* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2776c1c",
   "metadata": {},
   "source": [
    "### 2C. Analogies "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2a8668",
   "metadata": {},
   "source": [
    "For this part, your implementation will return a word `bb` that completes the analogy `a:b → aa:bb`. You will take as input three words, `a`, `b`, `aa`, and a list of candidate choices and return the choice that completes the analogy. For example,\n",
    "\n",
    "```\n",
    "man is to king as woman is to ___?  \n",
    "  a) wife \n",
    "  b) queen  \n",
    "  c) head  \n",
    "  d) ruler \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2cb3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_analogy_word(a: str, b: str, aa: str, \n",
    "                      choices: List[str], embeddings) -> str:\n",
    "    \"\"\"\n",
    "    Find the word bb that completes the analogy: a:b -> aa:bb\n",
    "    \n",
    "    For example, man:king -> woman:queen\n",
    "\n",
    "    Note: use cosine similarity as your similarity metric\n",
    "\n",
    "    Arguments:\n",
    "        - a, b, aa (str): words in the analogy described above\n",
    "        - choices (List[str]): list of strings for possible answer\n",
    "        - embeddings (Dict[str, np.array]): map of words to their embeddings\n",
    "\n",
    "    Returns:\n",
    "        - str: the word bb that completes the analogy\n",
    "    \"\"\"\n",
    "    # TODO: implement your solution here \n",
    "    # CODE START\n",
    "    raise NotImplementedError(\"Solution not yet implemented!\") #delete this line and add your solution\n",
    "    # CODE END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabc0e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample function call \n",
    "find_analogy_word(\"man\", \"king\",\"woman\", \n",
    "                [\"wife\",\"queen\",\"head\",\"ruler\"], embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7782dae7",
   "metadata": {},
   "source": [
    "#### Evaluating on 25 multiple choice questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f04eb5a",
   "metadata": {},
   "source": [
    "The code cell below runs your implementation on 25 multiple choice questions about **analogies**. \n",
    "\n",
    "Our reference implementation achieved\n",
    "- Accuracy = `0.64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bfcec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell \n",
    "part2C = quizlet.Part2C_Runner(find_analogy_word)\n",
    "_ = part2C.evaluate(True) # To only print the scores, pass in False as an argument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f51570a",
   "metadata": {},
   "source": [
    "### 2D. Sentence Similarity "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5609eac0",
   "metadata": {},
   "source": [
    "For this part, your goal is to input two sentences and output a similaity metric for those two sentences. You will do this in two parts. First, you will create a *sentence* embedding by adding the embeddings for all words in a sentence. Then you will evaluate the cosine similarity between two sentence embeddings. \n",
    "\n",
    "Once you have the cosine similarity between two sentence embeddings, our evaluation script does the following classification:\n",
    "- Classify two sentences as **similar** if their cosine similarity is `>0.95`\n",
    "- Else, classify the two sentences as **not similar**. \n",
    "\n",
    "Obtaining sentence embeddings by adding all the embeddings for the words in the sentence is the simplest approach to create sentence embeddings. Other approaches -- which you can try in extra credit -- include: \n",
    "- Weighting word embeddings by their part of speech tags (e.g. noun, verb, or adjective)\n",
    "- More clever combination of word embeddings (e.g. [Arora et al.](https://oar.princeton.edu/bitstream/88435/pr1rk2k/1/BaselineSentenceEmbedding.pdf))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88bc123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(s: str, embeddings: Dict[str, np.array])-> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns a embedding for a given sentence by adding all the \n",
    "    embeddings for the tokens in that sentence. \n",
    "    \n",
    "    This function IGNORES words that are not in our embeddings dictionary. \n",
    "    \n",
    "    Arguments:\n",
    "        - s (str): sentence\n",
    "        - embeddings (Dict[str, np.array]): map of words (strings) to their embeddings (np.array)\n",
    "        \n",
    "    Returns:\n",
    "        - np.ndarray: vector embedding of sentence s \n",
    "        \n",
    "    Hints: \n",
    "        - Remember how we tokenized using nltk and regex's?. You may \n",
    "        want to write a helper function for this.\n",
    "        - The vector you return should be the same shape as every other embedding \n",
    "        in our embeddings dictionary. \n",
    "        - Remember to skip words for which we do not have an embedding. We gave you some \n",
    "        hints for how to do this in the data exploration section. \n",
    "    \"\"\"\n",
    "    # TODO: implement your solution here \n",
    "    # CODE START\n",
    "    raise NotImplementedError(\"Solution not yet implemented!\") #delete this line and add your solution\n",
    "    # CODE END\n",
    "\n",
    "def get_similarity(s1: str, s2: str, embeddings: Dict[str, np.array]) -> float:\n",
    "    \"\"\"\n",
    "    Given 2 sentences and the embeddings dictionary, convert the sentences\n",
    "    into sentence embeddings and return the cosine similarity between them.\n",
    "\n",
    "    Arguments:\n",
    "        - s1, s2 (str): sentences\n",
    "        - embeddings (Dict[str, np.array]): map of words (strings) to their embeddings (np.array)\n",
    "\n",
    "    Returns:\n",
    "        - float: cosine similarity of the two sentence embeddings\n",
    "        \n",
    "    Hints: \n",
    "        - You should call get_sentence_embedding() somewhere in this function \n",
    "    \"\"\"\n",
    "    # TODO: implement your solution here \n",
    "    # CODE START\n",
    "    raise NotImplementedError(\"Solution not yet implemented!\") #delete this line and add your solution\n",
    "    # CODE END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40dd490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does your implemenation work on the toy examples below? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0938b8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"a man is doing trick.\"\n",
    "out = get_sentence_embedding(sentence, embeddings)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd8f5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"a man is doing trick.\"\n",
    "sent2 = \"the magician is doing a show.\"\n",
    "sent3 = \"you hate puppies\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ae6c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similarity(sent1, sent2, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b482cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similarity(sent1, sent3, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65c1f72",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8eee2f",
   "metadata": {},
   "source": [
    "The code cell below runs your implementation on 200 True/False questions about **sentence similarity**. \n",
    "\n",
    "Our reference implementation achieved\n",
    "- Accuracy = `0.78`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24937f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "part2D = quizlet.Part2D_Runner(get_similarity)\n",
    "_ = part2D.evaluate(True) # To only print the scores, pass in False as an argument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b868851",
   "metadata": {},
   "source": [
    "### 2E. Gendered occupations \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2224aec",
   "metadata": {},
   "source": [
    "In this part, given a list of occupations, you'll find the top 5 occupations with the highest cosine similarity to the word \"man\", and the top 5 occupations with the highest cosine similarity to the word \"woman\". \n",
    "\n",
    "Then you'll write a reflection about what you find. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d574de4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def occupation_exploration(occupations: List[str], \n",
    "                           embeddings: Dict[str, np.array]) -> Dict[str, List[Tuple[str, float]]]:\n",
    "    \"\"\"\n",
    "    Given a list of occupations, return the 5 occupations that are closest\n",
    "    to 'man', and the 5 closest to 'woman', using cosine similarity between\n",
    "    corresponding word embeddings as a measure of similarity.\n",
    "\n",
    "    Arguments:\n",
    "        - occupations (List[str]): list of occupations\n",
    "        - embeddings (Dict[str, np.array]): map of words (strings) to their embeddings (np.array)\n",
    "\n",
    "    Returns:\n",
    "        - dict: keys are strings \"man\" and \"woman\"\n",
    "                values are a list of tuples with the top 5 closest occupations\n",
    "                    first element in the tuple is the string of the occupation \n",
    "                    second element in the tuple is the cosine similarity \n",
    "                \n",
    "            The list of tuples should be sorted, with the occupation with highest\n",
    "                cosine similarity first in the list\n",
    "                \n",
    "    Hints: \n",
    "        - Feel free to write helper functions here (e.g. for the ranking)\n",
    "    \"\"\"\n",
    "    # TODO: implement your solution here \n",
    "    # CODE START\n",
    "    raise NotImplementedError(\"Solution not yet implemented!\") #delete this line and add your solution\n",
    "    # CODE END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65228fe9",
   "metadata": {},
   "source": [
    "#### Toy example to test the types of your function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a53f9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations = ['artist', 'engineer', 'driver', 'doctor', 'lawyer', \n",
    "               'teacher', 'homemaker', 'hairdresser', 'secretary', 'nurse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac6f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = occupation_exploration(occupations, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d15514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit tests: make sure your function outputs the correct type \n",
    "assert type(out) == dict\n",
    "assert set(out.keys()) == set([\"man\", \"woman\"])\n",
    "assert type(out[\"man\"]) == list\n",
    "assert type(out[\"man\"][0]) == tuple\n",
    "assert type(out[\"man\"][0][0]) == str\n",
    "tuple2 = out[\"man\"][0][1]\n",
    "assert type(tuple2) == float or isinstance(tuple2, np.floating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cabd64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you're only outputing the top 5 occupations \n",
    "assert len(out['man']) == 5\n",
    "assert len(out['woman']) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6cc8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "part2E = quizlet.Part2E_Runner(occupation_exploration)\n",
    "_ = part2E.evaluate() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28a8282",
   "metadata": {},
   "source": [
    "#### (5 points) Manual Analysis \n",
    "\n",
    "Take a look at what occupations you found are closest to \"man\" and\n",
    "closest to \"woman\". What do you notice? In *at least two complete sentences*, \n",
    "describe what you found, and why you think this occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa458d1c",
   "metadata": {},
   "source": [
    "*DELETE AND PUT YOUR ANSWER HERE* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a3ca03",
   "metadata": {},
   "source": [
    "### (Optional) 3. Extra credit \n",
    "*Extra credit can only help you and will not hurt you. At the end of the semester, if you have a borderline grade, your grade could be increased given your efforts on extra credit. This section is intended to be open-ended and challenge you. We suggest you only attempt this section after you have completed all other parts and are satisifed with your submission.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39564de5",
   "metadata": {},
   "source": [
    "**Sugessions for extra credit.** \n",
    "Improve the sentence embeddings function above. Our reference implementation was able to improve the accuracy of the sentence similarity questions to over `0.88` for the dev set. Feel free to implement one or more of the following.  \n",
    "\n",
    "**POS weighting.** Take a weighted sum of the individual word vectors, where the weighting depends on the part of speech (POS) of that given word. Using the dev data given, learn a different scalar weight for each POS tag (i.e., verb, noun, adjective, etc). Then multiply each word vector by the scalar weight associated with its part of speech. Finally, sum these weighted vectors to obtain a sentence embedding. \n",
    "\n",
    "*Hints:*\n",
    "- You can obtain POS tags via `tagged_words = nltk.pos_tag(word_tokens)`. Read more from the [NLTK documentation](https://www.nltk.org/book/ch05.html). \n",
    "        \n",
    "      \n",
    "**Arora method.** Implement the [Arora method](https://oar.princeton.edu/bitstream/88435/pr1rk2k/1/BaselineSentenceEmbedding.pdf) to obtain better sentence embeddings. \n",
    "\n",
    "**Alternative embeddings.**\n",
    "Swap out [different dense embeddings](https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models) and compare accuracies on the tasks above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a820ffa",
   "metadata": {},
   "source": [
    "**Instructions for extra credit submission.**\n",
    "We’re separating the extra credit from the normal submission so that (1) your extra credit does not affect your normal submission and (2) we do not break the memory limits on the Gradescope autograder.\n",
    "\n",
    "To sumbit: \n",
    "1. Create a new jupyter notebook (.ipynb) file.\n",
    "2. Write all your extra credit in this file.\n",
    "3. Once you’re done, in the top menu bar make sure to `Kernel -> Restart -> RunAll`.\n",
    "4. In the top menu bar, select` File -> Download as -> PDF via Latex (.pdf)`\n",
    "5. Upload this `.pdf` to Gradescope under the appropriate extra credit assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafcf682",
   "metadata": {},
   "source": [
    "## Submission "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efb1e91",
   "metadata": {},
   "source": [
    "**Processing reporting.** Please record your answers to the questions below by writing directly in this Markdown cell.\n",
    "\n",
    "If you talked with any of your classmates on this assignment please list their names here:\n",
    "\n",
    "*DELETE AND PUT YOUR ANSWER HERE.*\n",
    "\n",
    "Approximately how much time did you spend on this assignment:\n",
    "\n",
    "*DELETE AND PUT YOUR ANSWER HERE.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e6543b",
   "metadata": {},
   "source": [
    "**Download zip.** Once you're satsified with your solution, save this file and run the cell below to automatically zip your file. This will produce `submission.zip` in the same folder as this file (same folder as `hw4.ipynb`). \n",
    "\n",
    "Submit `submission.zip` to Gradescope. \n",
    "\n",
    "*Note:* This script assumes that you have the `zip` utility installed and you can use `bash` on your system. If the cell below does not work you may need to zip your file manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096636c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [[ ! -f \"./hw4.ipynb\" ]]\n",
    "then\n",
    "    echo \"WARNING: Did not find notebook in Jupyter working directory. Manual solution: go to File->Download .ipynb to download your notebok and other files, then zip them locally.\"\n",
    "else\n",
    "    echo \"Found notebook file, creating submission zip...\"\n",
    "    zip -r submission.zip hw4.ipynb\n",
    "fi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs375] *",
   "language": "python",
   "name": "conda-env-cs375-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
